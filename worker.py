import logging
import random
import socket
import traceback
import warnings
from builtins import object, str
from datetime import datetime
from typing import Optional, Tuple, Dict, TYPE_CHECKING

import pandas as pd
from sklearn.base import BaseEstimator, is_classifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_predict

from constants import AlgorithmStatus
from database import Database, Algorithm
from methods import ALGORITHMS
from utilities import ensure_directory, multiclass_roc_auc_score, logloss

if TYPE_CHECKING:
    from core import Core

warnings.filterwarnings('ignore')

LOGGER = logging.getLogger('mlb')
HOSTNAME = socket.gethostname()


# Exception thrown when something goes wrong for the worker, but the worker handles the error.
class AlgorithmError(Exception):
    pass


class Worker(object):
    def __init__(self,
                 database: Database,
                 dataset,
                 core,
                 cloud_mode: bool = False,
                 s3_access_key: str = None,
                 s3_secret_key: str = None,
                 s3_bucket: str = None,
                 models_dir: str = 'models',
                 metrics_dir: str = 'metrics',
                 verbose_metrics: bool = False):

        self.db = database
        self.dataset = dataset
        self.core: Core = core
        self.cloud_mode = cloud_mode

        self.s3_access_key = s3_access_key
        self.s3_secret_key = s3_secret_key
        self.s3_bucket = s3_bucket

        self.models_dir = models_dir
        self.metrics_dir = metrics_dir
        self.verbose_metrics = verbose_metrics
        ensure_directory(self.models_dir)
        ensure_directory(self.metrics_dir)

        """
        Load the Dataset from the database
        """
        self.dataset = self.db.get_dataset(self.dataset.id)

    def transform_dataset(self, algorithm: BaseEstimator) -> Tuple[pd.DataFrame, Dict[str, float]]:
        """
        Given a set of fully-qualified hyperparameters, create and test a
        algorithm model.
        Returns: Model object and metrics dictionary
        """

        """Load input dataset and class_column"""
        df = self.dataset.load()
        class_column = self.dataset.class_column

        """Split input dataset in X and y"""
        X, y = df.drop(class_column, axis=1), df[class_column]

        """
        Checks if algorithm (BaseEstimator) is a classifier. 
        
        If True, split X, y in train & test data, fit the algorithm model and call predict. Then calculate the
        evaluation metrics for the algorithm model and return them as a dict.
        
        If False, call fit_transform on X, y and return the transformed dataset as Dataframe.
        """

        if is_classifier(algorithm):
            """Predict labels with 5 fold cross validation"""
            # TODO muss auch objects handeln kÃ¶nnen
            y_pred = cross_val_predict(algorithm, X, y, cv=5)
            # -> nach einem kompletten durchlauf von iris ValueError: could not convert string to float: 'Iris-setosa'
            # -> LabelBinarizer()? siehe utilities.logloss

            # TODO switch/if else ob multiclass oder nicht
            # --> multiclass
            accuracy = accuracy_score(y, y_pred)
            precision = precision_score(y, y_pred, average='macro')
            # av_precision = average_precision(y, y_pred)
            recall = recall_score(y, y_pred, average='macro')
            f1 = f1_score(y, y_pred, average='macro')
            log_loss = logloss(y, y_pred)
            roc_auc = multiclass_roc_auc_score(y, y_pred, average='macro')

            # --> not multiclass

            """Convert np array y_pred to pd series and add it to X"""
            y_pred = pd.Series(y_pred)
            X = pd.concat([X, y_pred], axis=1)

            return X, {'accuracy': accuracy,
                       'precision': precision,
                       # 'av_precision': av_precision,
                       'recall': recall,
                       'f1': f1,
                       'neg_log_loss': log_loss,
                       'roc_auc': roc_auc
                       }
        else:
            """Call fit_transform on X, y and safe the transformed dataset in X"""
            X = algorithm.fit_transform(X, y)
            return X, {}

    def save_algorithm(self, algorithm_id: Optional[int], res: Tuple[pd.DataFrame, Dict[str, float]]) -> None:
        """
        Update a algorithm with metrics and model information and mark it as
        "complete"

        algorithm_id: ID of the algorithm to save

        model: Model object containing a serializable representation of the
            final model generated by this algorithm.

        metrics: Dictionary containing cross-validation and test metrics data
            for the model.
        """
        """Load input dataset and class_column. Drop class_column from input dataset."""
        dataset = self.dataset.load()
        class_column = self.dataset.class_column
        dataset, dataset_class_column = dataset.drop(class_column, axis=1), dataset[class_column]

        """Call complete_algorithm to save the algorithm to the database."""
        self.db.complete_algorithm(algorithm_id=algorithm_id, dataset=self.dataset, **res[1])
        LOGGER.info('Saved algorithm {}.'.format(algorithm_id))

        """Check if transformed dataset res[0] equals input dataset. If False store transformed dataset to DB"""
        if res[0].equals(dataset):
            LOGGER.info('Transformed dataset equals input dataset {} and is not stored in the DB.'
                        .format(self.dataset.id))

        else:
            """Load class_column and join transformed dataset with removed class_column of the input dataset.
            Add transformed dataset to DB"""
            class_column = self.dataset.class_column
            new_dataset = pd.concat([res[0], dataset_class_column], axis=1)
            depth = self.dataset.depth  # unsupported operand type(s) for +: 'NoneType' and 'int'
            depth += 1
            self.core.add_dataset(new_dataset, class_column, depth=depth)
            LOGGER.info('Transformed dataset will be stored in DB.')

    def is_dataset_finished(self):
        """
        Check if dataset is finished

        First is_dataset_finished checks if there are algorithms for this dataset in the database marked as pending or
        started. If there are none it returns False.

        Then is_dataset_finished checks if a dataset has enough budget for all the algorithms in the list.
        If the dataset has run out of budget, is_data_set returns True.
        """
        algorithms = self.db.get_algorithms(dataset_id=self.dataset.id, ignore_complete=False)
        # No algorithms for this data set started yet
        if not algorithms:
            return False

        n_completed = len(algorithms)
        if n_completed >= self.dataset.budget:
            LOGGER.warning('Algorithm budget for dataset {} has run out!'.format(self.dataset))
            return True

        if self.dataset.depth >= 5:
            LOGGER.warning('Dataset {} has reached max depth!'.format(self.dataset))
            return True

        return False

    def run_algorithm(self):
        """
        Check to see if our work is done

        First run_algorithm checks if is_dataset_finished returns True or False. If it returns True, the dataset is
        marked as complete. If is_dataset_finished returns False, run_algorithm creates a new Algorithm instance with
        a random Algorithm method. A random set of parameter configurations then is created for the Algorithms
        hyperparameter and stored in the new Algorithm instance.

        With the start_algorithm method, the new Algorithm instance then is stored in the database.

        As a last step the methods run_algorithm calls the methods transform_dataset and save_algorithm.
        """
        if self.is_dataset_finished():
            """
            Mark the run as done successfully
            """
            self.db.mark_dataset_complete(self.dataset.id)
            LOGGER.warning('Dataset {} has been marked as complete.'.format(self.dataset))
            return

        """
        Choose a random algorithm to work on the dataset
        """
        try:
            LOGGER.debug('Choosing algorithm...')
            algorithm = Algorithm(random.choice(ALGORITHMS),
                                  dataset_id=self.dataset.id,
                                  hyperparameter_values=None,
                                  status=AlgorithmStatus.RUNNING,
                                  start_time=datetime.now())

            """Save a random configuration of the algorithms hyperparameters in params"""
            params = algorithm.random_config()
            algorithm.hyperparameter_values = params

        except Exception:
            LOGGER.error('Error choosing hyperparameters: dataset={}'.format(self.dataset))
            LOGGER.error(traceback.format_exc())
            raise AlgorithmError()

        param_info = 'Chose parameters for algorithm "{}":'.format(algorithm.class_path)
        for k in sorted(params.keys()):
            param_info += '\n\t{} = {}'.format(k, params[k])
        LOGGER.info(param_info)

        LOGGER.debug('Creating algorithm...')

        """
        Start the algorithm (add it to database)
        """
        algorithm = self.db.start_algorithm(dataset_id=self.dataset.id,
                                            host=HOSTNAME,
                                            algorithm=algorithm,
                                            start_time=algorithm.start_time,
                                            status=algorithm.status)

        """
        Transform the dataset and save the algorithm
        """
        try:
            LOGGER.debug('Testing algorithm...')
            res = self.transform_dataset(algorithm.instance(params))

            LOGGER.debug('Saving algorithm...')
            self.save_algorithm(algorithm.id, res)

        except Exception:
            msg = traceback.format_exc()
            LOGGER.error('Error testing algorithm: dataset={}'.format(self.dataset))
            LOGGER.error(msg)
            self.db.mark_algorithm_errored(algorithm.id, error_message=msg)
            raise AlgorithmError()
